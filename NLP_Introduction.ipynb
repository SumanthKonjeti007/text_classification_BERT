{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D84Di8LM5w3e",
        "outputId": "08713a7b-81d6-4271-f3e7-b2118b50dc51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ],
      "metadata": {
        "id": "zwtGRgcW5_RZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"One Piece (stylized in all caps) is a Japanese manga series written and illustrated by Eiichiro Oda. It has been serialized in Shueisha's shōnen manga magazine Weekly Shōnen Jump since July 1997, with its individual chapters compiled in 108 tankōbon volumes as of March 2024. The story follows the adventures of Monkey D. Luffy and his crew, the Straw Hat Pirates, where he explores the Grand Line in search of the mythical treasure known as the One Piece in order to become the next King of the Pirates.\"\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wA1WS_0V6BnN",
        "outputId": "1edaac26-a802-4e79-ecd0-de09528989b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**"
      ],
      "metadata": {
        "id": "n5REx3FL7BRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Word tokenization\n",
        "words = word_tokenize(corpus)\n",
        "print(\"Words:\", words)\n",
        "\n",
        "# Sentence tokenization\n",
        "sentences = sent_tokenize(corpus)\n",
        "print(\"Sentences:\", sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YngWPzkw6H_N",
        "outputId": "1c4becfe-b745-44fd-9ae9-95238470f41a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words: ['One', 'Piece', '(', 'stylized', 'in', 'all', 'caps', ')', 'is', 'a', 'Japanese', 'manga', 'series', 'written', 'and', 'illustrated', 'by', 'Eiichiro', 'Oda', '.', 'It', 'has', 'been', 'serialized', 'in', 'Shueisha', \"'s\", 'shōnen', 'manga', 'magazine', 'Weekly', 'Shōnen', 'Jump', 'since', 'July', '1997', ',', 'with', 'its', 'individual', 'chapters', 'compiled', 'in', '108', 'tankōbon', 'volumes', 'as', 'of', 'March', '2024', '.', 'The', 'story', 'follows', 'the', 'adventures', 'of', 'Monkey', 'D.', 'Luffy', 'and', 'his', 'crew', ',', 'the', 'Straw', 'Hat', 'Pirates', ',', 'where', 'he', 'explores', 'the', 'Grand', 'Line', 'in', 'search', 'of', 'the', 'mythical', 'treasure', 'known', 'as', 'the', 'One', 'Piece', 'in', 'order', 'to', 'become', 'the', 'next', 'King', 'of', 'the', 'Pirates', '.']\n",
            "Sentences: ['One Piece (stylized in all caps) is a Japanese manga series written and illustrated by Eiichiro Oda.', \"It has been serialized in Shueisha's shōnen manga magazine Weekly Shōnen Jump since July 1997, with its individual chapters compiled in 108 tankōbon volumes as of March 2024.\", 'The story follows the adventures of Monkey D. Luffy and his crew, the Straw Hat Pirates, where he explores the Grand Line in search of the mythical treasure known as the One Piece in order to become the next King of the Pirates.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TreebankWordTokenizer\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "tokens = tokenizer.tokenize(corpus)\n",
        "print(tokens)\n",
        "# '.' will be considered as single word like doing. but except for the last."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQDDAiLA6Oov",
        "outputId": "9b4ffbe0-b4e0-4ed8-c7e8-18564b94e964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['One', 'Piece', '(', 'stylized', 'in', 'all', 'caps', ')', 'is', 'a', 'Japanese', 'manga', 'series', 'written', 'and', 'illustrated', 'by', 'Eiichiro', 'Oda.', 'It', 'has', 'been', 'serialized', 'in', 'Shueisha', \"'s\", 'shōnen', 'manga', 'magazine', 'Weekly', 'Shōnen', 'Jump', 'since', 'July', '1997', ',', 'with', 'its', 'individual', 'chapters', 'compiled', 'in', '108', 'tankōbon', 'volumes', 'as', 'of', 'March', '2024.', 'The', 'story', 'follows', 'the', 'adventures', 'of', 'Monkey', 'D.', 'Luffy', 'and', 'his', 'crew', ',', 'the', 'Straw', 'Hat', 'Pirates', ',', 'where', 'he', 'explores', 'the', 'Grand', 'Line', 'in', 'search', 'of', 'the', 'mythical', 'treasure', 'known', 'as', 'the', 'One', 'Piece', 'in', 'order', 'to', 'become', 'the', 'next', 'King', 'of', 'the', 'Pirates', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalization**\n",
        "\n",
        "Normalization typically includes converting all text to lowercase, removing punctuation, and other such cleaning steps."
      ],
      "metadata": {
        "id": "nwEaFFGqP85u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "# text = corpus\n",
        "lower_text = corpus.lower()\n",
        "clean_text = lower_text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "print(\"Normalized Text:\", clean_text)\n",
        "print(\"Character Count:\", len(clean_text))\n",
        "print(\"Word Count:\", len(clean_text.split()))\n",
        "\n",
        "# updated_corpus = clean_text\n",
        "# print(updated_corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR4XrVtjP8Co",
        "outputId": "766f69a2-dee3-42d3-efa3-8939c1c31a36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized Text: one piece stylized in all caps is a japanese manga series written and illustrated by eiichiro oda it has been serialized in shueishas shōnen manga magazine weekly shōnen jump since july 1997 with its individual chapters compiled in 108 tankōbon volumes as of march 2024 the story follows the adventures of monkey d luffy and his crew the straw hat pirates where he explores the grand line in search of the mythical treasure known as the one piece in order to become the next king of the pirates\n",
            "Character Count: 494\n",
            "Word Count: 88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stop Words Removal**\n",
        "\n",
        "Removing common words that might not be useful in analyzing text.\n",
        "\n",
        "It is really important to create your own stopwords, as there are few stopwords which are really important as per the usecase.\n",
        "\n"
      ],
      "metadata": {
        "id": "31IdU8zPRz7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Removing stopwords\n",
        "print(stopwords.words('english'))\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in word_tokenize(clean_text) if word not in stop_words]\n",
        "\n",
        "print(\"Filtered Words:\", filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKpuRfubR5ZM",
        "outputId": "cd37d9bc-234a-4747-c38c-a8e54b544e72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "Filtered Words: ['one', 'piece', 'stylized', 'caps', 'japanese', 'manga', 'series', 'written', 'illustrated', 'eiichiro', 'oda', 'serialized', 'shueishas', 'shōnen', 'manga', 'magazine', 'weekly', 'shōnen', 'jump', 'since', 'july', '1997', 'individual', 'chapters', 'compiled', '108', 'tankōbon', 'volumes', 'march', '2024', 'story', 'follows', 'adventures', 'monkey', 'luffy', 'crew', 'straw', 'hat', 'pirates', 'explores', 'grand', 'line', 'search', 'mythical', 'treasure', 'known', 'one', 'piece', 'order', 'become', 'next', 'king', 'pirates']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming**\n",
        "\n",
        "Reducing the word to it's word stem that affixes to suffixes and prefixes or to roots of the words know as lemma\n",
        "\n",
        "eg: Eating, gaten, ate - stem word is eat"
      ],
      "metadata": {
        "id": "eGp5jLKo726N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Stemming\n",
        "for words in filtered_words:\n",
        "  print(words+\"----->\"+ stemmer.stem(words))\n",
        "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
        "print(\"Stemmed Words:\", stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRxzcixb7T9l",
        "outputId": "578c3f76-a75a-47cd-cbd8-c660433f804a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "one----->one\n",
            "piece----->piec\n",
            "stylized----->styliz\n",
            "caps----->cap\n",
            "japanese----->japanes\n",
            "manga----->manga\n",
            "series----->seri\n",
            "written----->written\n",
            "illustrated----->illustr\n",
            "eiichiro----->eiichiro\n",
            "oda----->oda\n",
            "serialized----->serial\n",
            "shueishas----->shueisha\n",
            "shōnen----->shōnen\n",
            "manga----->manga\n",
            "magazine----->magazin\n",
            "weekly----->weekli\n",
            "shōnen----->shōnen\n",
            "jump----->jump\n",
            "since----->sinc\n",
            "july----->juli\n",
            "1997----->1997\n",
            "individual----->individu\n",
            "chapters----->chapter\n",
            "compiled----->compil\n",
            "108----->108\n",
            "tankōbon----->tankōbon\n",
            "volumes----->volum\n",
            "march----->march\n",
            "2024----->2024\n",
            "story----->stori\n",
            "follows----->follow\n",
            "adventures----->adventur\n",
            "monkey----->monkey\n",
            "luffy----->luffi\n",
            "crew----->crew\n",
            "straw----->straw\n",
            "hat----->hat\n",
            "pirates----->pirat\n",
            "explores----->explor\n",
            "grand----->grand\n",
            "line----->line\n",
            "search----->search\n",
            "mythical----->mythic\n",
            "treasure----->treasur\n",
            "known----->known\n",
            "one----->one\n",
            "piece----->piec\n",
            "order----->order\n",
            "become----->becom\n",
            "next----->next\n",
            "king----->king\n",
            "pirates----->pirat\n",
            "Stemmed Words: ['one', 'piec', 'styliz', 'cap', 'japanes', 'manga', 'seri', 'written', 'illustr', 'eiichiro', 'oda', 'serial', 'shueisha', 'shōnen', 'manga', 'magazin', 'weekli', 'shōnen', 'jump', 'sinc', 'juli', '1997', 'individu', 'chapter', 'compil', '108', 'tankōbon', 'volum', 'march', '2024', 'stori', 'follow', 'adventur', 'monkey', 'luffi', 'crew', 'straw', 'hat', 'pirat', 'explor', 'grand', 'line', 'search', 'mythic', 'treasur', 'known', 'one', 'piec', 'order', 'becom', 'next', 'king', 'pirat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the disadvantage of stemming, as it don't have any meaning of the word\n",
        "\n",
        "example: compiled----->compil\n",
        "\n",
        "we use this stemming, when we have classifation problem like spam/ham"
      ],
      "metadata": {
        "id": "W-gfeyjHUjdr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RegexpStemmer**"
      ],
      "metadata": {
        "id": "Y6EHV3wFVZTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer\n",
        "regexp_stemmer = RegexpStemmer('ing$|s$|e$|able$', min=4)"
      ],
      "metadata": {
        "id": "8-RFBC1gSmOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Snowball Stemmer\n",
        "\n",
        "A better version of porter stemmer, it gives a better version of words went compared to other stemmers."
      ],
      "metadata": {
        "id": "dOH_Zr9mWPq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snowball_stemmer = SnowballStemmer('english')\n",
        "for words in filtered_words:\n",
        "  print(words+\"----->\"+ snowball_stemmer.stem(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YL6QLLZPV6KH",
        "outputId": "ef56b3af-453e-49d0-9018-878eab7276f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "one----->one\n",
            "piece----->piec\n",
            "stylized----->styliz\n",
            "caps----->cap\n",
            "japanese----->japanes\n",
            "manga----->manga\n",
            "series----->seri\n",
            "written----->written\n",
            "illustrated----->illustr\n",
            "eiichiro----->eiichiro\n",
            "oda----->oda\n",
            "serialized----->serial\n",
            "shueishas----->shueisha\n",
            "shōnen----->shōnen\n",
            "manga----->manga\n",
            "magazine----->magazin\n",
            "weekly----->week\n",
            "shōnen----->shōnen\n",
            "jump----->jump\n",
            "since----->sinc\n",
            "july----->juli\n",
            "1997----->1997\n",
            "individual----->individu\n",
            "chapters----->chapter\n",
            "compiled----->compil\n",
            "108----->108\n",
            "tankōbon----->tankōbon\n",
            "volumes----->volum\n",
            "march----->march\n",
            "2024----->2024\n",
            "story----->stori\n",
            "follows----->follow\n",
            "adventures----->adventur\n",
            "monkey----->monkey\n",
            "luffy----->luffi\n",
            "crew----->crew\n",
            "straw----->straw\n",
            "hat----->hat\n",
            "pirates----->pirat\n",
            "explores----->explor\n",
            "grand----->grand\n",
            "line----->line\n",
            "search----->search\n",
            "mythical----->mythic\n",
            "treasure----->treasur\n",
            "known----->known\n",
            "one----->one\n",
            "piece----->piec\n",
            "order----->order\n",
            "become----->becom\n",
            "next----->next\n",
            "king----->king\n",
            "pirates----->pirat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer.stem(\"fairly\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vfmQMkEKWw1P",
        "outputId": "0edeacf1-916b-4c5d-e978-20b54a61c874"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'fairli'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "snowball_stemmer.stem(\"fairly\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KnsCYxRgXhyy",
        "outputId": "e70b633a-a1d2-40b9-b6e0-5bc446032435"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'fair'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Lemmatization\n",
        "\n",
        "We get the root word exactly not the stem. It's more accurate than tokenization.\n",
        "\n",
        "We can use this for chatbots, Q&A, text summerization."
      ],
      "metadata": {
        "id": "-CB8cy5PXvrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "'''\n",
        "POS\n",
        "Noun - n\n",
        "Verb - v\n",
        "Adjective - a\n",
        "Adverb - r\n",
        "'''\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "print(\"Lemmatized Words:\", lemmatized_words)"
      ],
      "metadata": {
        "id": "OeIOv6_xXly_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47362452-f33f-458a-f532-77e2ca39810b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Words: ['one', 'piece', 'stylized', 'cap', 'japanese', 'manga', 'series', 'written', 'illustrated', 'eiichiro', 'oda', 'serialized', 'shueishas', 'shōnen', 'manga', 'magazine', 'weekly', 'shōnen', 'jump', 'since', 'july', '1997', 'individual', 'chapter', 'compiled', '108', 'tankōbon', 'volume', 'march', '2024', 'story', 'follows', 'adventure', 'monkey', 'luffy', 'crew', 'straw', 'hat', 'pirate', 'explores', 'grand', 'line', 'search', 'mythical', 'treasure', 'known', 'one', 'piece', 'order', 'become', 'next', 'king', 'pirate']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cKA84GS1TKpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"\"\"\n",
        "\"One Piece\" is a widely acclaimed anime and manga series created by Eiichiro Oda. It debuted in 1997 and quickly rose to immense popularity, captivating audiences worldwide with its thrilling adventures and complex storytelling. The series follows Monkey D. Luffy, a spirited young pirate with the ability to stretch his body like rubber due to consuming a Devil Fruit. Luffy's dream is to find the legendary treasure known as \"One Piece\" and become the Pirate King. Accompanied by his diverse and lovable crew, the Straw Hat Pirates, Luffy travels across the Grand Line, encountering other pirates, bounty hunters, and various other characters, each with their own unique abilities and backstories. \"One Piece\" is celebrated for its intricate plot, deep character development, and a perfect blend of humor, action, and drama, making it a cornerstone of anime culture.\"\"\"\n"
      ],
      "metadata": {
        "id": "KmRzH9O3TLGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "4ZUOltgWTWo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmHrpJ2ITWrZ",
        "outputId": "1a328bf2-770d-4070-c3f0-3a3eb27b7f03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "5CzmeMy3TsSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.sent_tokenize(corpus)"
      ],
      "metadata": {
        "id": "HuXF8mQKTy9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Apply stopwords and filter and then Apply stemming\n",
        "for i in range(len(sentences)):\n",
        "  words = nltk.word_tokenize(sentences[i])\n",
        "  words = [stemmer.stem(word) for word in words if word not in stopwords.words('english')]\n",
        "  sentences[i] = ' '.join(words)"
      ],
      "metadata": {
        "id": "eQv47xXpT2wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xy0cZlSRUVPp",
        "outputId": "19b324fa-9bf2-4c5d-98c2-5bc1dcb6affa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"'' one piec '' wide acclaim anim manga seri creat eiichiro oda .\",\n",
              " 'it debut 1997 quickli rose immens popular , captiv audienc worldwid thrill adventur complex storytel .',\n",
              " 'the seri follow monkey d. luffi , spirit young pirat abil stretch bodi like rubber due consum devil fruit .',\n",
              " \"luffi 's dream find legendari treasur known `` one piec '' becom pirat king .\",\n",
              " 'accompani divers lovabl crew , straw hat pirat , luffi travel across grand line , encount pirat , bounti hunter , variou charact , uniqu abil backstori .',\n",
              " \"`` one piec '' celebr intric plot , deep charact develop , perfect blend humor , action , drama , make cornerston anim cultur .\"]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snowball_stemmer = SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "NaLlivGJUW85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Apply stopwords and filter and then Apply stemming\n",
        "for i in range(len(sentences)):\n",
        "  words = nltk.word_tokenize(sentences[i])\n",
        "  words = [snowball_stemmer.stem(word) for word in words if word not in stopwords.words('english')]\n",
        "  sentences[i] = ' '.join(words)\n",
        "\n",
        "#here in snowball it will make the starting word to lower case."
      ],
      "metadata": {
        "id": "a2Vk1ikaUhfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucLpvCp1UkwN",
        "outputId": "efa73e7a-5bfb-4297-f49d-f2116adb2e0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"'' one piec `` wide acclaim anim manga seri creat eiichiro oda .\",\n",
              " 'debut 1997 quick rose immen popular , captiv audienc worldwid thrill adventur complex storytel .',\n",
              " 'seri follow monkey d. luffi , spirit young pirat abil stretch bodi like rubber due consum devil fruit .',\n",
              " \"luffi 's dream find legendari treasur known `` one piec `` becom pirat king .\",\n",
              " 'accompani diver lovabl crew , straw hat pirat , luffi travel across grand line , encount pirat , bounti hunter , variou charact , uniqu abil backstori .',\n",
              " '`` one piec `` celebr intric plot , deep charact develop , perfect blend humor , action , drama , make cornerston anim cultur .']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saohROCMUl0L",
        "outputId": "2912e905-6a5c-4e84-bb6d-6b65e29713ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(sentences)):\n",
        "  words = nltk.word_tokenize(sentences[i])\n",
        "  words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in stopwords.words('english')]\n",
        "  sentences[i] = ' '.join(words)"
      ],
      "metadata": {
        "id": "S65ZWz-sVGSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjYlSOIAVJBk",
        "outputId": "377b4846-8392-482c-c335-bf1d2e7554ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"'' one piec `` wide acclaim anim manga seri creat eiichiro oda .\",\n",
              " 'debut 1997 quick rose immen popular , captiv audienc worldwid thrill adventur complex storytel .',\n",
              " 'seri follow monkey d. luffi , spirit young pirat abil stretch bodi like rubber due consum devil fruit .',\n",
              " \"luffi 's dream find legendari treasur known `` one piec `` becom pirat king .\",\n",
              " 'accompani diver lovabl crew , straw hat pirat , luffi travel across grand line , encount pirat , bounti hunter , variou charact , uniqu abil backstori .',\n",
              " '`` one piec `` celebr intric plot , deep charact develop , perfect blend humor , action , drama , make cornerston anim cultur .']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Now this parts of speech tagging plays an important role in lemmatization.***\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "'''\n",
        "POS\n",
        "Noun - n\n",
        "Verb - v\n",
        "Adjective - a\n",
        "Adverb - r\n",
        "'''\n",
        "\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in filtered_words]"
      ],
      "metadata": {
        "id": "niakgGzuWcg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"\"\"\n",
        "\"One Piece\" is a widely acclaimed anime and manga series created by Eiichiro Oda. It debuted in 1997 and quickly rose to immense popularity, captivating audiences worldwide with its thrilling adventures and complex storytelling. The series follows Monkey D. Luffy, a spirited young pirate with the ability to stretch his body like rubber due to consuming a Devil Fruit. Luffy's dream is to find the legendary treasure known as \"One Piece\" and become the Pirate King. Accompanied by his diverse and lovable crew, the Straw Hat Pirates, Luffy travels across the Grand Line, encountering other pirates, bounty hunters, and various other characters, each with their own unique abilities and backstories. \"One Piece\" is celebrated for its intricate plot, deep character development, and a perfect blend of humor, action, and drama, making it a cornerstone of anime culture.\"\"\"\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "sentences=nltk.sent_tokenize(corpus)"
      ],
      "metadata": {
        "id": "o5IQbIdJVmUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQ7zMfFurI8C",
        "outputId": "57461b1c-9f6b-4162-fb56-34d3ca64708e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## We will find the Pos Tag\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "    words=nltk.word_tokenize(sentences[i])\n",
        "    words=[word for word in words if word not in set(stopwords.words('english'))]\n",
        "    #sentences[i]=' '.join(words)# converting all the list of words into sentences\n",
        "    pos_tag=nltk.pos_tag(words)\n",
        "    print(pos_tag)"
      ],
      "metadata": {
        "id": "gthlItwGrNFk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52425858-f5b0-47d7-a86a-5f3bd3c3aa14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(\"''\", \"''\"), ('One', 'CD'), ('Piece', 'NNP'), (\"''\", \"''\"), ('widely', 'RB'), ('acclaimed', 'VBD'), ('anime', 'JJ'), ('manga', 'NN'), ('series', 'NN'), ('created', 'VBD'), ('Eiichiro', 'NNP'), ('Oda', 'NNP'), ('.', '.')]\n",
            "[('It', 'PRP'), ('debuted', 'VBD'), ('1997', 'CD'), ('quickly', 'RB'), ('rose', 'VBD'), ('immense', 'JJ'), ('popularity', 'NN'), (',', ','), ('captivating', 'VBG'), ('audiences', 'NNS'), ('worldwide', 'RB'), ('thrilling', 'VBG'), ('adventures', 'NNS'), ('complex', 'JJ'), ('storytelling', 'NN'), ('.', '.')]\n",
            "[('The', 'DT'), ('series', 'NN'), ('follows', 'VBZ'), ('Monkey', 'NNP'), ('D.', 'NNP'), ('Luffy', 'NNP'), (',', ','), ('spirited', 'VBD'), ('young', 'JJ'), ('pirate', 'NN'), ('ability', 'NN'), ('stretch', 'VBP'), ('body', 'NN'), ('like', 'IN'), ('rubber', 'NN'), ('due', 'JJ'), ('consuming', 'VBG'), ('Devil', 'NNP'), ('Fruit', 'NNP'), ('.', '.')]\n",
            "[('Luffy', 'NNP'), (\"'s\", 'POS'), ('dream', 'NN'), ('find', 'VBP'), ('legendary', 'JJ'), ('treasure', 'NN'), ('known', 'VBN'), ('``', '``'), ('One', 'CD'), ('Piece', 'NNP'), (\"''\", \"''\"), ('become', 'VB'), ('Pirate', 'NNP'), ('King', 'NNP'), ('.', '.')]\n",
            "[('Accompanied', 'NNP'), ('diverse', 'NN'), ('lovable', 'JJ'), ('crew', 'NN'), (',', ','), ('Straw', 'NNP'), ('Hat', 'NNP'), ('Pirates', 'NNP'), (',', ','), ('Luffy', 'NNP'), ('travels', 'VBZ'), ('across', 'IN'), ('Grand', 'NNP'), ('Line', 'NNP'), (',', ','), ('encountering', 'VBG'), ('pirates', 'NNS'), (',', ','), ('bounty', 'NN'), ('hunters', 'NNS'), (',', ','), ('various', 'JJ'), ('characters', 'NNS'), (',', ','), ('unique', 'JJ'), ('abilities', 'NNS'), ('backstories', 'NNS'), ('.', '.')]\n",
            "[('``', '``'), ('One', 'CD'), ('Piece', 'NNP'), (\"''\", \"''\"), ('celebrated', 'VBD'), ('intricate', 'JJ'), ('plot', 'NN'), (',', ','), ('deep', 'JJ'), ('character', 'NN'), ('development', 'NN'), (',', ','), ('perfect', 'JJ'), ('blend', 'NN'), ('humor', 'NN'), (',', ','), ('action', 'NN'), (',', ','), ('drama', 'NN'), (',', ','), ('making', 'VBG'), ('cornerstone', 'NN'), ('anime', 'JJ'), ('culture', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Named Entity Recognition"
      ],
      "metadata": {
        "id": "tm-ITEsIwDQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence=\"The Eiffel Tower was built from 1887 to 1889 by Gustave Eiffel, whose company specialized in building metal frameworks and structures.\""
      ],
      "metadata": {
        "id": "rAjSnSZLrfN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "words=nltk.word_tokenize(sentence)"
      ],
      "metadata": {
        "id": "1UCrl0UJwKh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_elements=nltk.pos_tag(words)"
      ],
      "metadata": {
        "id": "lNQc9fZpwL_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('maxent_ne_chunker')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sonMCuV-wNA7",
        "outputId": "9967c5fa-c04a-4c0f-bd06-29fd3623270e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XASuQDoLwODs",
        "outputId": "d92442d0-02b5-4a97-ccfd-5bc58d7c464e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.ne_chunk(tag_elements).draw()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "0AApCirEwQIe",
        "outputId": "b8445e9b-4a22-49a5-8466-7c01d816b2b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TclError",
          "evalue": "no display name and no $DISPLAY environment variable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-bbdad5f7ef3a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mne_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_elements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tree/tree.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdraw_trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m         \u001b[0mdraw_trees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpretty_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhighlight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/draw/tree.py\u001b[0m in \u001b[0;36mdraw_trees\u001b[0;34m(*trees)\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m     \"\"\"\n\u001b[0;32m-> 1008\u001b[0;31m     \u001b[0mTreeView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1009\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/draw/tree.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *trees)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_top\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_top\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NLTK\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_top\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<Control-x>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/tkinter/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, screenName, baseName, className, useTk, sync, use)\u001b[0m\n\u001b[1;32m   2297\u001b[0m                 \u001b[0mbaseName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2298\u001b[0m         \u001b[0minteractive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2299\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tkinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreenName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwantobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2300\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2301\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loadtk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTclError\u001b[0m: no display name and no $DISPLAY environment variable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text to Vector\n",
        "\n",
        "Bag of words - Used in sentiment analysis, small text classification\n",
        "\n",
        "Process:\n",
        "\n",
        "Sentences -> lower case -> stop words removal -> Unique words\n",
        "\n",
        "Now vocabulary and frequecy of that words\n",
        "\n",
        "With the frequecy of occurance we can consider the features as needed.\n",
        "\n",
        "Now vectors are constructed based on the features.\n",
        "\n",
        "If the word occured more than once, we can increase it's count accordingly\n",
        "\n",
        "\n",
        "*   Binary Bag of words - Even it has more than one occurance it forces to have only one.\n",
        "*   Bag of words - you can increase the count to more than one as per it's frequecy of occurance\n",
        "\n",
        "Sentence - \"good boy good\"\n",
        "Features - good boy girl => is acheived by frequency of occureance in the doc\n",
        "\n",
        "1.   [1 1 0] - Binary BOW\n",
        "2.   [2 1 0] - BOW\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "88vBhREnPYmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample text\n",
        "corpus = \"\"\"\n",
        "\"One Piece\" is a widely acclaimed anime and manga series created by Eiichiro Oda. It debuted in 1997 and quickly rose to immense popularity, captivating audiences worldwide with its thrilling adventures and complex storytelling. The series follows Monkey D. Luffy, a spirited young pirate with the ability to stretch his body like rubber due to consuming a Devil Fruit. Luffy's dream is to find the legendary treasure known as \"One Piece\" and become the Pirate King. Accompanied by his diverse and lovable crew, the Straw Hat Pirates, Luffy travels across the Grand Line, encountering other pirates, bounty hunters, and various other characters, each with their own unique abilities and backstories. \"One Piece\" is celebrated for its intricate plot, deep character development, and a perfect blend of humor, action, and drama, making it a cornerstone of anime culture.\"\"\"\n",
        "\n",
        "sentences = nltk.sent_tokenize(corpus)\n",
        "\n",
        "# Apply stopwords and filter and then Apply lemmatizer\n",
        "for i in range(len(sentences)):\n",
        "  words = nltk.word_tokenize(sentences[i])\n",
        "  words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in stopwords.words('english')]\n",
        "  sentences[i] = ' '.join(words)\n",
        "\n",
        "# Creating the BoW model\n",
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform(sentences)\n",
        "\n",
        "# Output the vocabulary and the BoW feature matrix\n",
        "print(\"Vocabulary:\", vectorizer.vocabulary_)\n",
        "print(\"Bag of Words Matrix:\\n\", bow_matrix.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9coIkxUtwRnE",
        "outputId": "7ac2a328-5f31-4b85-b8c4-c4568988d57e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: {'one': 54, 'piece': 56, 'widely': 74, 'acclaimed': 2, 'anime': 7, 'manga': 51, 'series': 63, 'created': 20, 'eiichiro': 31, 'oda': 53, 'it': 42, 'debuted': 23, '1997': 0, 'quickly': 60, 'rose': 61, 'immense': 40, 'popularity': 59, 'captivating': 14, 'audience': 8, 'worldwide': 75, 'thrilling': 69, 'adventure': 6, 'complex': 17, 'storytelling': 65, 'the': 68, 'follows': 34, 'monkey': 52, 'luffy': 49, 'spirited': 64, 'young': 76, 'pirate': 57, 'ability': 1, 'stretch': 67, 'body': 12, 'like': 46, 'rubber': 62, 'due': 30, 'consuming': 18, 'devil': 26, 'fruit': 35, 'dream': 29, 'find': 33, 'legendary': 45, 'treasure': 71, 'known': 44, 'become': 10, 'king': 43, 'accompanied': 3, 'diverse': 27, 'lovable': 48, 'crew': 21, 'straw': 66, 'hat': 37, 'travel': 70, 'across': 4, 'grand': 36, 'line': 47, 'encountering': 32, 'bounty': 13, 'hunter': 39, 'various': 73, 'character': 16, 'unique': 72, 'backstories': 9, 'celebrated': 15, 'intricate': 41, 'plot': 58, 'deep': 24, 'development': 25, 'perfect': 55, 'blend': 11, 'humor': 38, 'action': 5, 'drama': 28, 'making': 50, 'cornerstone': 19, 'culture': 22}\n",
            "Bag of Words Matrix:\n",
            " [[0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
            "  0 0 1 0 0]\n",
            " [1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0 0 0 1 0 0\n",
            "  0 0 0 1 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1\n",
            "  0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0\n",
            "  0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0\n",
            "  0 0 0 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
            "  0 0 0 0 0]\n",
            " [0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0\n",
            "  1 1 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 1 0 0 0 1 0\n",
            "  1 1 0 0 0]\n",
            " [0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0\n",
            "  0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Sematic meaning like the importance of the word is not captured.\n",
        "\n",
        "\n",
        "Example :\n",
        "\n",
        "sentence1 - The food is good\n",
        "\n",
        "sentence2 - The food is not good\n",
        "\n",
        "\n",
        "- Not removing stop words, Vocabulary - The, food, is, not, good\n",
        "\n",
        "- Vectors - [1, 1, 1, 0, 1], [1, 1, 1, 1, 1]\n",
        "\n",
        "The consine similarity will be very similar like the angle between them will be very less, it's like both have similar meaning\n",
        "But, it's not true.\n",
        "\n",
        "- Sparse matrix or array leads to overfitting\n",
        "- Ording of the word is getting changed.\n",
        "- Out of vocabulary"
      ],
      "metadata": {
        "id": "OdTu1YaSdmJa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Ngrams\n",
        "\n",
        "Used for text generation is required, such as chatbots, predictive typing aids, etc."
      ],
      "metadata": {
        "id": "2_Bupf36hoAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"Hello world, welcome to natural language processing.\"\n",
        "\n",
        "# Tokenizing the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Generating Bigrams\n",
        "bigrams = list(ngrams(tokens, 2))\n",
        "print(\"Bigrams:\", bigrams)\n",
        "\n",
        "# Generating Trigrams\n",
        "trigrams = list(ngrams(tokens, 3))\n",
        "print(\"Trigrams:\", trigrams)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3rno9LrcUMT",
        "outputId": "803cf222-cd54-4563-92c4-8f83cfc35372"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigrams: [('Hello', 'world'), ('world', ','), (',', 'welcome'), ('welcome', 'to'), ('to', 'natural'), ('natural', 'language'), ('language', 'processing'), ('processing', '.')]\n",
            "Trigrams: [('Hello', 'world', ','), ('world', ',', 'welcome'), (',', 'welcome', 'to'), ('welcome', 'to', 'natural'), ('to', 'natural', 'language'), ('natural', 'language', 'processing'), ('language', 'processing', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###TF-IDF (Term Frequency - Inverse Document Frequency)\n",
        "\n",
        "Term Frequency - (Number of repeation of the word in sentense/number of words in sentence)\n",
        "\n",
        "Inverse Document Freqeuncy - (Number of Sentences/Number of sentences the word contains)\n",
        "\n",
        "Vector = TF*IDF\n",
        "\n",
        "Main Imp advantage of this is the word importance is captured.\n",
        "\n",
        "TF-IDF scores represent the importance of a word to a document in a collection. It increases proportionally to the number of times a word appears in the document but is offset by the number of documents that contain the word.\n"
      ],
      "metadata": {
        "id": "LHfQJlC3nh5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "corpus = \"\"\"\n",
        "\"One Piece\" is a widely acclaimed anime and manga series created by Eiichiro Oda. It debuted in 1997 and quickly rose to immense popularity, captivating audiences worldwide with its thrilling adventures and complex storytelling. The series follows Monkey D. Luffy, a spirited young pirate with the ability to stretch his body like rubber due to consuming a Devil Fruit. Luffy's dream is to find the legendary treasure known as \"One Piece\" and become the Pirate King. Accompanied by his diverse and lovable crew, the Straw Hat Pirates, Luffy travels across the Grand Line, encountering other pirates, bounty hunters, and various other characters, each with their own unique abilities and backstories. \"One Piece\" is celebrated for its intricate plot, deep character development, and a perfect blend of humor, action, and drama, making it a cornerstone of anime culture.\"\"\"\n",
        "\n",
        "sentences = nltk.sent_tokenize(corpus)\n",
        "print(sentences)\n",
        "# Creating the TF-IDF model\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n",
        "\n",
        "# Output the vocabulary and the TF-IDF feature matrix\n",
        "print(\"Vocabulary:\", tfidf_vectorizer.vocabulary_)\n",
        "print(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sIUbF_wh6BW",
        "outputId": "d23eed88-774a-4e95-ab7c-6b8baba8da90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n\"One Piece\" is a widely acclaimed anime and manga series created by Eiichiro Oda.', 'It debuted in 1997 and quickly rose to immense popularity, captivating audiences worldwide with its thrilling adventures and complex storytelling.', 'The series follows Monkey D. Luffy, a spirited young pirate with the ability to stretch his body like rubber due to consuming a Devil Fruit.', 'Luffy\\'s dream is to find the legendary treasure known as \"One Piece\" and become the Pirate King.', 'Accompanied by his diverse and lovable crew, the Straw Hat Pirates, Luffy travels across the Grand Line, encountering other pirates, bounty hunters, and various other characters, each with their own unique abilities and backstories.', '\"One Piece\" is celebrated for its intricate plot, deep character development, and a perfect blend of humor, action, and drama, making it a cornerstone of anime culture.']\n",
            "Vocabulary: {'one': 66, 'piece': 70, 'is': 51, 'widely': 91, 'acclaimed': 3, 'anime': 9, 'and': 8, 'manga': 62, 'series': 78, 'created': 25, 'by': 17, 'eiichiro': 37, 'oda': 64, 'it': 52, 'debuted': 28, 'in': 49, '1997': 0, 'quickly': 75, 'rose': 76, 'to': 86, 'immense': 48, 'popularity': 74, 'captivating': 18, 'audiences': 11, 'worldwide': 93, 'with': 92, 'its': 53, 'thrilling': 85, 'adventures': 7, 'complex': 22, 'storytelling': 80, 'the': 83, 'follows': 40, 'monkey': 63, 'luffy': 60, 'spirited': 79, 'young': 94, 'pirate': 71, 'ability': 2, 'stretch': 82, 'his': 45, 'body': 15, 'like': 57, 'rubber': 77, 'due': 35, 'consuming': 23, 'devil': 31, 'fruit': 42, 'dream': 34, 'find': 39, 'legendary': 56, 'treasure': 88, 'known': 55, 'as': 10, 'become': 13, 'king': 54, 'accompanied': 4, 'diverse': 32, 'lovable': 59, 'crew': 26, 'straw': 81, 'hat': 44, 'pirates': 72, 'travels': 87, 'across': 5, 'grand': 43, 'line': 58, 'encountering': 38, 'other': 67, 'bounty': 16, 'hunters': 47, 'various': 90, 'characters': 21, 'each': 36, 'their': 84, 'own': 68, 'unique': 89, 'abilities': 1, 'backstories': 12, 'celebrated': 19, 'for': 41, 'intricate': 50, 'plot': 73, 'deep': 29, 'character': 20, 'development': 30, 'perfect': 69, 'blend': 14, 'of': 65, 'humor': 46, 'action': 6, 'drama': 33, 'making': 61, 'cornerstone': 24, 'culture': 27}\n",
            "TF-IDF Matrix:\n",
            " [[0.         0.         0.         0.32078912 0.         0.\n",
            "  0.         0.         0.16434884 0.26305167 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.26305167\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.32078912 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.32078912 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.22208629 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.32078912 0.         0.32078912 0.\n",
            "  0.22208629 0.         0.         0.         0.22208629 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.26305167 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.32078912 0.         0.         0.        ]\n",
            " [0.24005363 0.         0.         0.         0.         0.\n",
            "  0.         0.24005363 0.24597178 0.         0.         0.24005363\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.24005363 0.         0.         0.         0.24005363 0.\n",
            "  0.         0.         0.         0.         0.24005363 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.24005363 0.24005363 0.         0.         0.19684741 0.19684741\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.24005363 0.24005363 0.24005363 0.\n",
            "  0.         0.         0.24005363 0.         0.         0.\n",
            "  0.         0.24005363 0.16619211 0.         0.         0.\n",
            "  0.         0.         0.16619211 0.24005363 0.        ]\n",
            " [0.         0.         0.22467522 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.22467522 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.22467522\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.22467522 0.         0.         0.         0.22467522\n",
            "  0.         0.         0.         0.         0.22467522 0.\n",
            "  0.22467522 0.         0.         0.1842369  0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.22467522 0.         0.\n",
            "  0.15554545 0.         0.         0.22467522 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.1842369\n",
            "  0.         0.         0.         0.         0.         0.22467522\n",
            "  0.1842369  0.22467522 0.         0.         0.22467522 0.3110909\n",
            "  0.         0.         0.3110909  0.         0.         0.\n",
            "  0.         0.         0.15554545 0.         0.22467522]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.14075457 0.         0.27473595 0.\n",
            "  0.         0.27473595 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.27473595 0.\n",
            "  0.         0.         0.         0.27473595 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.19020311 0.         0.\n",
            "  0.27473595 0.27473595 0.27473595 0.         0.         0.\n",
            "  0.19020311 0.         0.         0.         0.         0.\n",
            "  0.19020311 0.         0.         0.         0.19020311 0.22528741\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.38040622\n",
            "  0.         0.         0.19020311 0.         0.27473595 0.\n",
            "  0.         0.         0.         0.         0.        ]\n",
            " [0.         0.16764057 0.         0.         0.16764057 0.16764057\n",
            "  0.         0.         0.25766023 0.         0.         0.\n",
            "  0.16764057 0.         0.         0.         0.16764057 0.13746766\n",
            "  0.         0.         0.         0.16764057 0.         0.\n",
            "  0.         0.         0.16764057 0.         0.         0.\n",
            "  0.         0.         0.16764057 0.         0.         0.\n",
            "  0.16764057 0.         0.16764057 0.         0.         0.\n",
            "  0.         0.16764057 0.16764057 0.13746766 0.         0.16764057\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.16764057 0.16764057\n",
            "  0.11605965 0.         0.         0.         0.         0.\n",
            "  0.         0.33528114 0.16764057 0.         0.         0.\n",
            "  0.33528114 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.16764057 0.         0.2321193\n",
            "  0.16764057 0.         0.         0.16764057 0.         0.16764057\n",
            "  0.16764057 0.         0.11605965 0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.20626198 0.         0.21134705 0.16913777 0.         0.\n",
            "  0.         0.         0.20626198 0.         0.         0.\n",
            "  0.         0.20626198 0.20626198 0.         0.         0.\n",
            "  0.20626198 0.         0.         0.20626198 0.         0.20626198\n",
            "  0.20626198 0.         0.         0.20626198 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.20626198\n",
            "  0.         0.         0.         0.         0.20626198 0.\n",
            "  0.         0.         0.20626198 0.14279773 0.16913777 0.16913777\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.20626198 0.         0.         0.         0.41252396\n",
            "  0.14279773 0.         0.         0.20626198 0.14279773 0.\n",
            "  0.         0.20626198 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Word Embeddings\n",
        "\n",
        "Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation. They are a set of feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers in a low-dimensional space relative to the vocabulary size.\n",
        "\n",
        "Concept: Word embeddings are learned in a way that reflects the semantic and syntactic similarities between words. For instance, words like \"king\" and \"queen\" will have embeddings that are closer to each other than to embeddings for unrelated words like \"apple.\"\n",
        "\n",
        "Two thing what Word2Vec gives is the, very good representation of word to vector and gives the similar meaning to the words which are closer in vector space.\n",
        "\n",
        "**The word meaning is highlighed only with the efficient word conversion to vector**\n",
        "\n",
        "All one-hot encoding, tf-idf and bag of words are part of Word Embeddings.\n",
        "With the correct word conversion to vector, we can find the exact similarity between the words more accuratly.\n",
        "\n",
        "Two Types:\n",
        "\n",
        "\n",
        "*   Count / Frequency [One-hot encoding, Bag of Words, TF-IDF]\n",
        "*   Deep leaning trained models [Word2Vec] (More accurate)\n",
        "\n",
        "Word2Vec uses a Neural Network model to learn **Word associations from large corpus**\n",
        "\n",
        "Once trained it can detect synonyous words or suggests additional words for a partial sentence.\n",
        "\n",
        "Word2Vec represents a distict word with a particular list or numbers called vectors.\n",
        "\n",
        "\n",
        "*   Continous Bag of words\n",
        "*   Skipgram\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SdK6V0RT1fzR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see zeros in all the embeddings it includes (Bag of words, TI-IDF). But, in case of Word2Vec we have somewhat different\n",
        "\n",
        "Vocabulary -> Unique words in corpus\n",
        "\n",
        "***Each word inside Vocabulary will be converted into FEATURE REPRESENTATION***\n",
        "\n",
        "The Numerical number is assigned based on the Unique word and the features.\n",
        "Features like (Gender, Royal,...etc) n dimensions\n",
        "\n",
        "More the window size more better the model performs.\n",
        "\n",
        "At the end of the day, each Unique word in vocubulary is represented in n dimensions vector.\n",
        "\n",
        "Recommendations also comes here."
      ],
      "metadata": {
        "id": "EKT9TpxcQiRn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Under **Word2Vec** we have two Continous Bag of words, skipgrams. Parallely, we have **Pretrained Models(Google), Train a model from scratch**\n"
      ],
      "metadata": {
        "id": "ACy1SKtMlqkU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CBOW - Continous Bag of Words**\n",
        "\n",
        "Prediction - Focus word\n",
        "\n",
        "**SkipGram**\n",
        "\n",
        "Prediction - Context word\n",
        "\n"
      ],
      "metadata": {
        "id": "andQOk-hmZZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjixHKsNv4iw",
        "outputId": "c67c4e4a-2183-4b7a-cc8c-41debf510905"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(\"Current Working Directory:\", os.getcwd())\n",
        "print(\"Files in the Directory:\", os.listdir())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVE1QSmrThRx",
        "outputId": "a02f4cab-0835-407e-d8f4-d3f3290bfe43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Working Directory: /content\n",
            "Files in the Directory: ['.config', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load the Word2Vec model directly from Gensim\n",
        "word_vectors = api.load(\"word2vec-google-news-300\")  # This loads the model if it's not already downloaded\n",
        "\n",
        "# Check if the model is loaded correctly\n",
        "print(\"Loaded word vectors:\", type(word_vectors))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVHkUzGpVDsn",
        "outputId": "e5210896-8677-4684-eb80-3a310faa608e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
            "Loaded word vectors: <class 'gensim.models.keyedvectors.KeyedVectors'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Load the Word2Vec model directly from Gensim\n",
        "word_vectors = api.load(\"word2vec-google-news-300\")  # This loads the model if it's not already downloaded\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZ4BPQpkKNbe",
        "outputId": "fdb68434-c0ed-4f3d-eba3-462a5b7c3b9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "documents = [\"I love this movie\", \"I hate this movie\", \"This movie is great\", \"This movie is terrible\"]\n",
        "labels = [1, 0, 1, 0]  # 1 for positive, 0 for negative\n",
        "# Function to convert document to averaged word vector\n",
        "def document_vector(doc):\n",
        "    words = doc.split()\n",
        "    word_vectors_list = [word_vectors[word] for word in words if word in word_vectors]\n",
        "    return np.mean(word_vectors_list, axis=0) if word_vectors_list else np.zeros(300)\n",
        "\n",
        "# Convert documents to vectors and normalize\n",
        "X = np.array([document_vector(doc) for doc in documents])\n",
        "X = normalize(X)  # Normalize vectors\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.25, random_state=42)\n",
        "\n",
        "# Train the model with different regularization settings\n",
        "model = LogisticRegression(max_iter=1000, C=0.1)  # Try adjusting C\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "predictions = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(\"Predictions:\", predictions)\n",
        "print(\"True Labels:\", y_test)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utWDAbD1LEoJ",
        "outputId": "1e1fd43c-d913-4073-f2ac-7e8feb237a4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [1]\n",
            "True Labels: [0]\n",
            "Accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import normalize\n",
        "import numpy as np\n",
        "def document_vector(doc):\n",
        "    words = doc.split()\n",
        "    word_vectors_list = [word_vectors[word] for word in words if word in word_vectors]\n",
        "    return np.mean(word_vectors_list, axis=0) if word_vectors_list else np.zeros(300)\n",
        "\n",
        "# Convert documents to vectors and normalize\n",
        "X = np.array([document_vector(doc) for doc in documents])\n",
        "X = normalize(X)  # Normalize vectors\n",
        "\n",
        "# Labels\n",
        "labels = [1, 0, 1, 0]  # 1 for positive, 0 for negative\n",
        "\n",
        "# Use SVM and cross-validation\n",
        "model = SVC(kernel='linear')\n",
        "scores = cross_val_score(model, X, labels, cv=2)  # Using 5-fold cross-validation\n",
        "\n",
        "print(\"Cross-validated scores:\", scores)\n",
        "print(\"Average score:\", np.mean(scores))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxNdHwKzbcHb",
        "outputId": "53d65778-e09b-49b3-9122-ac93fffdb77a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validated scores: [1. 1.]\n",
            "Average score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense\n",
        "\n",
        "# Generate a simple sequence data\n",
        "# Example: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 -> Predict the next number in the sequence\n",
        "\n",
        "# Create training data\n",
        "def create_sequence(n_steps):\n",
        "    X, y = [], []\n",
        "    for i in range(len(n_steps)):\n",
        "        end_ix = i + 1\n",
        "        if end_ix > len(n_steps) - 1:\n",
        "            break\n",
        "        seq_x, seq_y = n_steps[i:end_ix], n_steps[end_ix]\n",
        "        X.append(seq_x)\n",
        "        y.append(seq_y)\n",
        "        print(X, y)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "raw_seq = np.array([i for i in range(10)])\n",
        "print(\"Raw_Seq\", raw_seq)\n",
        "X, y = create_sequence(raw_seq)\n",
        "print(\"X:\", X)\n",
        "print(\"y:\", y)\n",
        "\n",
        "# Reshape input to be [samples, time steps, features]\n",
        "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "print(\"X:\", X)\n",
        "\n",
        "# Define the RNN model\n",
        "model = Sequential([\n",
        "    SimpleRNN(50, activation='relu', input_shape=(X.shape[1], X.shape[2])),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=300, verbose=0)\n",
        "\n",
        "# Predict the next number\n",
        "test_input = np.array([13]).reshape((1, 1, 1))  # Input sequence: [8]\n",
        "print(test_input)\n",
        "predicted_number = model.predict(test_input, verbose=0)\n",
        "print(\"Predicted number:\", predicted_number)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eri9H1jjacnd",
        "outputId": "546c29c0-2e42-4c3c-c79a-630cf8ad3752"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw_Seq [0 1 2 3 4 5 6 7 8 9]\n",
            "[array([0])] [1]\n",
            "[array([0]), array([1])] [1, 2]\n",
            "[array([0]), array([1]), array([2])] [1, 2, 3]\n",
            "[array([0]), array([1]), array([2]), array([3])] [1, 2, 3, 4]\n",
            "[array([0]), array([1]), array([2]), array([3]), array([4])] [1, 2, 3, 4, 5]\n",
            "[array([0]), array([1]), array([2]), array([3]), array([4]), array([5])] [1, 2, 3, 4, 5, 6]\n",
            "[array([0]), array([1]), array([2]), array([3]), array([4]), array([5]), array([6])] [1, 2, 3, 4, 5, 6, 7]\n",
            "[array([0]), array([1]), array([2]), array([3]), array([4]), array([5]), array([6]), array([7])] [1, 2, 3, 4, 5, 6, 7, 8]\n",
            "[array([0]), array([1]), array([2]), array([3]), array([4]), array([5]), array([6]), array([7]), array([8])] [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "X: [[0]\n",
            " [1]\n",
            " [2]\n",
            " [3]\n",
            " [4]\n",
            " [5]\n",
            " [6]\n",
            " [7]\n",
            " [8]]\n",
            "y: [1 2 3 4 5 6 7 8 9]\n",
            "X: [[[0]]\n",
            "\n",
            " [[1]]\n",
            "\n",
            " [[2]]\n",
            "\n",
            " [[3]]\n",
            "\n",
            " [[4]]\n",
            "\n",
            " [[5]]\n",
            "\n",
            " [[6]]\n",
            "\n",
            " [[7]]\n",
            "\n",
            " [[8]]]\n",
            "[[[13]]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7c913e173be0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted number: [[14.485639]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gZdz_WD_ZAB6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}